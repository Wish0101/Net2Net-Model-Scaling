# Net2Net-Model-Scaling
A PyTorch implementation of Net2WiderNet for scaling up Transformer-based LLMs (e.g., Gemma, Llama). This toolkit allows you to widen MLP layers to increase parameter count while preserving model function, complete with scripts for calculation, validation, inference, and LoRA fine-tuning.
